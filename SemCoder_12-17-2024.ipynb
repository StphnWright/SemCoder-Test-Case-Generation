{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "HJZ1QzRd0sii",
        "nUHM4NGUFknM",
        "JDG6m5xoW2b1",
        "KzOoIRgm0ndG",
        "YaormCswUMDk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Generative Models for Code** -- Final Project<br><br>\n",
        "**Maria Gancayco (mig2131@columbia.edu)**<br>\n",
        "**Stephen Wright (svw2112@columbia.edu)**<br>\n",
        "*Due:* Wednesday, 12 Dec 2024 at 11:59pm ET"
      ],
      "metadata": {
        "id": "0YBqgsFTo1AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Setup"
      ],
      "metadata": {
        "id": "HJZ1QzRd0sii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: Environment and Memory Management\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# Check and display GPU availability for transparency\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
        "\n",
        "# Memory management utilities\n",
        "def clear_memory() -> None:\n",
        "    \"\"\"\n",
        "    Clears GPU memory cache and performs garbage collection.\n",
        "\n",
        "    This function is crucial for maintaining optimal memory usage during model evaluation,\n",
        "    especially when loading and comparing multiple large language models.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # Clear CUDA cache\n",
        "    gc.collect()  # Trigger Python garbage collection\n",
        "\n",
        "def get_memory_status() -> None:\n",
        "    \"\"\"\n",
        "    Displays current GPU memory usage statistics.\n",
        "\n",
        "    Reports both allocated and reserved memory in megabytes (MB).\n",
        "    This helps monitor memory consumption during model operations.\n",
        "\n",
        "    Note:\n",
        "        - Allocated memory: Actually used GPU memory\n",
        "        - Reserved memory: Total memory reserved by PyTorch\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Convert bytes to MB for better readability\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"GPU Memory: Allocated: {allocated:.2f}MB, Reserved: {reserved:.2f}MB\")\n",
        "clear_memory()\n",
        "# Initialize by checking current memory status\n",
        "get_memory_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgvmsDDzsLYC",
        "outputId": "a69f8fc6-e00c-4de0-c430-584eb2f93665",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device name: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: Allocated: 0.00MB, Reserved: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration and Setup\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Configuration dataclass containing all hyperparameters and settings for model evaluation.\n",
        "\n",
        "    Attributes:\n",
        "        model_name (str): Name/path of the model to be evaluated\n",
        "        batch_size (int): Number of samples processed in each batch\n",
        "        learning_rate (float): Learning rate for model optimization\n",
        "        num_epochs (int): Number of training epochs\n",
        "        max_seq_length (int): Maximum sequence length for input tokenization\n",
        "        gradient_accumulation_steps (int): Number of steps to accumulate gradients\n",
        "        warmup_steps (Optional[int]): Number of warmup steps for learning rate scheduler\n",
        "        weight_decay (float): L2 regularization factor\n",
        "        eval_steps (int): Frequency of evaluation steps\n",
        "        save_steps (int): Frequency of model checkpoint saves\n",
        "        logging_steps (int): Frequency of logging training metrics\n",
        "    \"\"\"\n",
        "    model_name: str\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    num_epochs: int\n",
        "    max_seq_length: int\n",
        "    gradient_accumulation_steps: int\n",
        "    warmup_steps: Optional[int] = None\n",
        "    weight_decay: float = 0.01\n",
        "    eval_steps: int = 100\n",
        "    save_steps: int = 100\n",
        "    logging_steps: int = 10\n",
        "\n",
        "# Set up results directory for storing evaluation outputs\n",
        "results_dir = Path(\"./results\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "print(\"Configuration and directories initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX4vwvGqs3-G",
        "outputId": "3256586f-1fcf-4ab3-abb9-9219338c6d89",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and directories initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = ExperimentConfig(\n",
        "    model_name=\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n",
        "    batch_size=1,                    # Small batch size due to model size\n",
        "    learning_rate=5e-5,             # Conservative learning rate for fine-tuning\n",
        "    num_epochs=3,                   # Number of training epochs\n",
        "    max_seq_length=512,            # Maximum sequence length for input processing\n",
        "    gradient_accumulation_steps=32, # Accumulate gradients to simulate larger batch size\n",
        "    warmup_steps=100               # Warmup steps for learning rate scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "BlGsbQiSJt4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Dependencies and Imports\n",
        "\n",
        "# Install core dependencies for transformer model handling and evaluation\n",
        "!pip install transformers torch timeout-decorator\n",
        "\n",
        "# Import required libraries\n",
        "import torch  # PyTorch for deep learning operations\n",
        "from transformers import (\n",
        "    AutoTokenizer,         # For tokenization of input text\n",
        "    AutoModelForCausalLM   # For loading pre-trained causal language models\n",
        ")\n",
        "from typing import List, Dict  # Type hints for better code documentation\n",
        "import timeout_decorator\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Y0TX5UXgwG1q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"openai_humaneval\")"
      ],
      "metadata": {
        "id": "APCk2qR2dedO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loading and Code Generation\n",
        "\n",
        "def load_model_and_tokenizer(config: ExperimentConfig) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "\n",
        "    try:\n",
        "        # Clear memory before loading new model to prevent OOM errors\n",
        "        clear_memory()\n",
        "\n",
        "        print(f\"Loading {config.model_name}...\")\n",
        "\n",
        "        # Initialize tokenizer with remote code execution enabled\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=True  # Required for custom tokenizer implementations\n",
        "        )\n",
        "\n",
        "        # Load model with memory-efficient settings\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16,    # Use bfloat16 for memory efficiency\n",
        "            device_map=\"auto\",             # Optimize model placement across available devices\n",
        "            low_cpu_mem_usage=True         # Minimize CPU memory during loading\n",
        "        )\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "            model.gradient_checkpointing_enable()  # Trade compute for memory savings\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        get_memory_status()  # Display current memory usage\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generate_code(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 50\n",
        ") -> str:\n",
        "\n",
        "    try:\n",
        "        # Format prompt as chat message\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        print(\"Generating inputs...\")\n",
        "        # Tokenize input with chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "        print(\"Generating outputs...\")\n",
        "        # Generate code with specified parameters\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_new_tokens,  # Control generation length\n",
        "            do_sample=True,                 # Enable sampling-based generation\n",
        "            temperature=temperature,         # Control randomness\n",
        "            top_p=top_p,                    # Nucleus sampling threshold\n",
        "            top_k=top_k,                    # Top-k sampling parameter\n",
        "            num_return_sequences=1,         # Generate single sequence\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode and return only the generated portion (excluding prompt)\n",
        "        return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in code generation: {str(e)}\")\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "zNeAngZKtFzY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and tokenizer using configuration\n",
        "deepseek_7b_model, deepseek_7b_tokenizer = load_model_and_tokenizer(config)"
      ],
      "metadata": {
        "id": "F66bJYxMJHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HumanEval+ Test Case Generation"
      ],
      "metadata": {
        "id": "nUHM4NGUFknM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def generate_humaneval_plus_tests(model_type, deepseek_model=None, deepseek_tokenizer=None, num_total_tests=100):\n",
        "    dataset = load_dataset(\"openai_humaneval\")\n",
        "    results = []\n",
        "    total_tests_generated = 0\n",
        "    with open(f'{model_type}_test_case_generation_results.txt', 'w') as f:\n",
        "      for i in range(len(dataset['test'])):\n",
        "          if total_tests_generated >= num_total_tests:\n",
        "              break\n",
        "\n",
        "          problem = dataset['test'][i]\n",
        "          prompt = problem['prompt']\n",
        "          solution = problem['canonical_solution']\n",
        "          entry_point = problem['entry_point']\n",
        "          test_code = problem['test']\n",
        "\n",
        "          # Extract working test cases\n",
        "          check_match = re.search(r'def check\\(candidate\\):\\s*(.*?)(?=\\n\\n|$)', test_code, re.DOTALL)\n",
        "          test_cases = re.findall(r'assert.*?(?=\\n|$)', check_match.group(1) if check_match else '')\n",
        "\n",
        "          test_prompt = f\"\"\"\n",
        "Please provide executable test cases for this function:\n",
        "{prompt}\n",
        "\n",
        "Working test examples:\n",
        "{test_cases}\n",
        "\n",
        "Include these types of tests:\n",
        "1. Performance test:\n",
        "def test_{entry_point}_perf():\n",
        "    {test_cases[0].replace('candidate', entry_point)}\n",
        "\n",
        "2. Edge case test:\n",
        "def test_{entry_point}_edge():\n",
        "    {test_cases[-1].replace('candidate', entry_point)}\n",
        "\n",
        "3. Error test:\n",
        "def test_{entry_point}_error():\n",
        "    with pytest.raises(TypeError):\n",
        "        {entry_point}(None)\n",
        "\n",
        "Only provide executable test cases. No placeholders.\"\"\"\n",
        "\n",
        "          try:\n",
        "              generated_tests, cleaned_tests = None, None\n",
        "              if model_type == \"semcoder\":\n",
        "                generated_tests = semcoder.generate_code(test_prompt)\n",
        "                cleaned_tests = evaluator.clean_generated_code(generated_tests)\n",
        "              elif \"deepseek\"in model_type:\n",
        "                generated_tests = generate_code(deepseek_model, deepseek_tokenizer, test_prompt, max_new_tokens=4096)\n",
        "                cleaned_tests = clean_deepseek_generated_code(generated_tests)\n",
        "              elif model_type == \"gpt-4\":\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                        {\"role\": \"user\", \"content\": test_prompt}\n",
        "                    ],\n",
        "                    max_tokens=4096,\n",
        "                    temperature=0.8,\n",
        "                    top_p=0.95\n",
        "                )\n",
        "                generated_tests = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "                cleaned_tests = clean_deepseek_generated_code(generated_tests) #TODO:- Please rename this since we're also using for OpenAI\n",
        "              if cleaned_tests:\n",
        "                  num_tests = len(re.findall(r'def test_', cleaned_tests))\n",
        "                  total_tests_generated += num_tests\n",
        "\n",
        "                  result = {\n",
        "                      'problem_id': i,\n",
        "                      'entry_point': entry_point,\n",
        "                      'tests': cleaned_tests,\n",
        "                      'num_tests': num_tests\n",
        "                  }\n",
        "                  results.append(result)\n",
        "\n",
        "                  print(f\"Generated {num_tests} enhanced tests\")\n",
        "                  print(f\"Total tests so far: {total_tests_generated}/{num_total_tests}\")\n",
        "                  print(\"\\nTest prompt:\")\n",
        "                  print(test_prompt)\n",
        "                  print(\"\\nGenerated tests:\")\n",
        "                  print(generated_tests)\n",
        "                  print(\"\\nCleaned tests:\")\n",
        "                  print(cleaned_tests)\n",
        "\n",
        "                  f.write(f\"Generated {num_tests} enhanced tests\\n\")\n",
        "                  f.write(f\"Total tests so far: {total_tests_generated}/{num_total_tests}\")\n",
        "                  f.write(\"\\nGenerated tests:\\n\")\n",
        "                  f.write(cleaned_tests + \"\\n\")\n",
        "              else:\n",
        "                  print(\"No valid tests generated\")\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"Error generating tests: {str(e)}\")\n",
        "              continue\n",
        "\n",
        "    return results, total_tests_generated"
      ],
      "metadata": {
        "id": "ok47e3T_IVNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate HumanEval+ tests for DeepSeek 7B\n",
        "print(\"Generating HumanEval+ test cases...\")\n",
        "plus_results, total_plus_tests = generate_humaneval_plus_tests(\"deepseek_7b\", deepseek_model=deepseek_7b_model, deepseek_tokenizer=deepseek_7b_tokenizer, num_total_tests=100)"
      ],
      "metadata": {
        "id": "ocMFO0cQK7gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate HumanEval+ tests for SemCoder\n",
        "print(\"Generating HumanEval+ test cases...\")\n",
        "plus_results, total_plus_tests = generate_humaneval_plus_tests(\"semcoder\", num_total_tests=100)"
      ],
      "metadata": {
        "id": "vCPKjhAA_5q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Utilities"
      ],
      "metadata": {
        "id": "6bwKickq6PzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_test_suites(content: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract test suites from the content and format them with function calls.\n",
        "    Handles both standalone assert statements and function definitions.\n",
        "    Returns a list of formatted test suite strings.\n",
        "    \"\"\"\n",
        "    # Split content into test suite blocks\n",
        "    test_blocks = re.split(r'Generated \\d+ enhanced tests\\nTotal tests so far: \\d+/\\d+\\n+Generated tests:', content)\n",
        "\n",
        "    # Remove empty blocks\n",
        "    test_blocks = [block.strip() for block in test_blocks if block.strip()]\n",
        "\n",
        "    formatted_suites = []\n",
        "    for block in test_blocks:\n",
        "        if \"unittest.TestCase\" in block:\n",
        "          print(\"FORMATTED TEST SUITE:\")\n",
        "          print(block)\n",
        "          formatted_suites.append(block)\n",
        "          continue\n",
        "\n",
        "\n",
        "        print(\"ORIGINAL TEST SUITE:\")\n",
        "        print(block)\n",
        "        suite_parts = []\n",
        "\n",
        "        # First, collect any imports at the start of the block\n",
        "        import_statements = re.findall(r'^import [^\\n]+', block, re.MULTILINE)\n",
        "\n",
        "        # Extract function-based tests\n",
        "        test_functions = re.finditer(r'def (test_\\w+)\\(\\):\\n((?:[ ]{4}.*\\n?)+)', block)\n",
        "\n",
        "        # Extract standalone assert statements (not within functions)\n",
        "        # Looking for asserts that are at the start of a line and not indented\n",
        "        standalone_asserts = re.finditer(r'^assert [^\\n]+$', block, re.MULTILINE)\n",
        "\n",
        "        # Extract standalone pytest.raises statements\n",
        "        standalone_raises = re.finditer(r'^with pytest\\.raises\\([^\\)]+\\):\\n[ ]{4}[^\\n]+\\n', block, re.MULTILINE)\n",
        "\n",
        "        # Add imports if they exist\n",
        "        if import_statements:\n",
        "            suite_parts.extend(import_statements)\n",
        "            suite_parts.append(\"\")  # Add blank line after imports\n",
        "\n",
        "        # Add standalone asserts\n",
        "        for match in standalone_asserts:\n",
        "            suite_parts.append(match.group(0))\n",
        "\n",
        "        # Add standalone pytest.raises\n",
        "        for match in standalone_raises:\n",
        "            suite_parts.append(match.group(0).rstrip())\n",
        "\n",
        "        # Add function-based tests\n",
        "        for match in test_functions:\n",
        "            func_name = match.group(1)\n",
        "            func_body = match.group(2).rstrip()\n",
        "            formatted_func = f\"def {func_name}():\\n{func_body}\\n{func_name}()\"\n",
        "            suite_parts.append(formatted_func)\n",
        "\n",
        "        if suite_parts:\n",
        "            formatted_suite = \"\\n\".join(suite_parts)\n",
        "            print(\"FORMATTED TEST SUITE:\")\n",
        "            print(formatted_suite)\n",
        "            print(\"-\" * 50)\n",
        "            formatted_suites.append(formatted_suite)\n",
        "\n",
        "    return formatted_suites\n",
        "\n",
        "def process_file_path(file_path: str) -> list[str]:\n",
        "    \"\"\"Process a file by path and return list of formatted test suite strings.\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "    return extract_test_suites(content)\n",
        "\n",
        "def process_file_content(content: str) -> list[str]:\n",
        "    \"\"\"Process file content directly and return list of formatted test suite strings.\"\"\"\n",
        "    return extract_test_suites(content)"
      ],
      "metadata": {
        "id": "WBeBJle2E4Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "def execute_test_case(code: str, test_case: str) -> bool:\n",
        "    try:\n",
        "        namespace = {}\n",
        "        # Execute the function code\n",
        "        exec(code, namespace)\n",
        "        # Execute the test case\n",
        "        exec(\"import pytest\", namespace)\n",
        "        exec(test_case, namespace)\n",
        "        return True\n",
        "    except pytest.raises.Exception:\n",
        "        # This catches when pytest.raises() fails (i.e., expected exception wasn't raised)\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions\n",
        "        return False\n",
        "\n",
        "def check_syntax(code: str) -> bool:\n",
        "        try:\n",
        "            compile(code, '<string>', 'exec')\n",
        "            return True\n",
        "        except SyntaxError:\n",
        "            return False\n",
        "@timeout_decorator.timeout(5)\n",
        "def evaluate_single_test_suite(solution: str,\n",
        "                               generated_tests: str) -> Dict:\n",
        "        syntax_valid = check_syntax(solution + \"\\n\" + generated_tests)\n",
        "\n",
        "        # Execute test cases if syntax is valid\n",
        "        if syntax_valid:\n",
        "            # TODO:- consider using thread pool for parallel test execution\n",
        "            execution_success = execute_test_case(solution, generated_tests)\n",
        "        else:\n",
        "            execution_success = False\n",
        "\n",
        "        return {\n",
        "            \"syntax_valid\": syntax_valid,\n",
        "            \"execution_success\": execution_success\n",
        "        }\n",
        "def evaluate_test_suite(model_type, dataset, n_tasks, test_suites):\n",
        "  solutions = dataset['test'][\"canonical_solution\"]\n",
        "  metrics = {\"syntax_validity\": 0.0,  # Syntactic correctness\n",
        "            \"execution_accuracy\": 0.0  # Functional correctness\n",
        "  }\n",
        "  results = []\n",
        "  with open(f'{model_type}_test_case_generation_accuracy_results.txt', 'w') as f:\n",
        "          for i in range(n_tasks):\n",
        "              solution = solutions[i]\n",
        "              full_solution = dataset['test'][\"prompt\"][i] + solution\n",
        "              cleaned_tests = test_suites[i]\n",
        "              result = evaluate_single_test_suite(full_solution, cleaned_tests)\n",
        "\n",
        "              f.write(f\"PROBLEM {i}:\\n\")\n",
        "              print(f\"PROBLEM {i}:\\n\")\n",
        "              f.write(\"CANONICAL SOLUTION:\\n\")\n",
        "              print(\"CANONICAL SOLUTION:\\n\")\n",
        "              f.write(full_solution + \"\\n\")\n",
        "              print(full_solution + \"\\n\")\n",
        "              f.write(\"CLEANED TESTS:\\n\")\n",
        "              print(\"CLEANED TESTS:\\n\")\n",
        "              f.write(cleaned_tests + \"\\n\")\n",
        "              print(cleaned_tests)\n",
        "              f.write(\"RESULT:\\n\" + str(result) + \"\\n\")\n",
        "              print(\"RESULT:\\n\" + str(result))\n",
        "\n",
        "              results.append(result)\n",
        "\n",
        "          metrics[\"syntax_validity\"] = np.mean([r[\"syntax_valid\"] for r in results])\n",
        "          metrics[\"execution_accuracy\"] = np.mean([r[\"execution_success\"] for r in results])\n",
        "          f.write(str(metrics))\n",
        "\n",
        "def clean_deepseek_generated_code(code: str) -> str:\n",
        "        \"\"\"Clean up generated code to extract only the functions.\"\"\"\n",
        "        lines = code.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        found_start = False\n",
        "        found_test_func_call = False\n",
        "        for line in lines:\n",
        "            if line.startswith('```python'):\n",
        "                found_start = True\n",
        "            elif line.startswith('```'):\n",
        "                if found_test_func_call: break\n",
        "                else: found_start = False\n",
        "            elif found_start:\n",
        "                if line.startswith('test_') and line.endswith('()'):\n",
        "                    found_test_func_call = True\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(cleaned_lines).strip()"
      ],
      "metadata": {
        "id": "qgB4E7XDH7xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Results: DeepSeek vs SemCoder"
      ],
      "metadata": {
        "id": "YfB82raABzRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_7b_extracted_test_suites = process_file_path(\"/content/deepseek_7b_test_case_generation_results.txt\")"
      ],
      "metadata": {
        "id": "roFIR_sMOHri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_extracted_test_suites = process_file_path(\"/content/semcoder_test_case_generation_results.txt\")"
      ],
      "metadata": {
        "id": "yaA_Si2n_2l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_test_suite(\"deepseek_7b\", dataset, len(deepseek_7b_extracted_test_suites), deepseek_7b_extracted_test_suites)"
      ],
      "metadata": {
        "id": "Pdj8aDaZdVsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_test_suite(\"semcoder\", dataset, len(semcoder_extracted_test_suites), semcoder_extracted_test_suites)"
      ],
      "metadata": {
        "id": "WUDAVHRCAOvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SemCoder Simple Prompt Results"
      ],
      "metadata": {
        "id": "JDG6m5xoW2b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_generated_code(code: str) -> str:\n",
        "    \"\"\"Clean up generated code to extract only the functions.\"\"\"\n",
        "    lines = code.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    in_function = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('def '):\n",
        "            in_function = True\n",
        "            cleaned_lines.append(line)\n",
        "        elif in_function and (line.startswith('    ') or not line.strip()):\n",
        "            cleaned_lines.append(line)\n",
        "        elif in_function and line.strip() and not line.startswith('    '):\n",
        "            in_function = False\n",
        "            cleaned_lines.append('')\n",
        "\n",
        "    return '\\n'.join(cleaned_lines).strip()"
      ],
      "metadata": {
        "id": "F5j6feh4XGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeek Simple Prompt Results"
      ],
      "metadata": {
        "id": "KzOoIRgm0ndG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "import timeout_decorator\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "OMHcCCL4bwbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "            \"pass@1\": 0.0,      # Single-attempt success rate\n",
        "            \"pass@10\": 0.0,     # Success within 10 attempts\n",
        "            \"pass@100\": 0.0,    # Success within 100 attempts\n",
        "            \"syntax_validity\": 0.0,  # Syntactic correctness\n",
        "            \"execution_accuracy\": 0.0  # Functional correctness\n",
        "}\n",
        "# TODO:- rename to accomodate that we're also using this for GPT-4\n",
        "\n",
        "def evaluate_model(model, dataset, model_type, tokenizer, n_tasks: int = None):\n",
        "        solutions = dataset['test'][\"canonical_solution\"]\n",
        "        if n_tasks is None:\n",
        "            n_tasks = len(solutions)\n",
        "\n",
        "        results = []\n",
        "        with open(f'{model_type}_test_case_generation_results.txt', 'w') as f:\n",
        "          for i in range(n_tasks):\n",
        "              solution = solutions[i]\n",
        "              full_solution = dataset['test'][\"prompt\"][i] + solution\n",
        "\n",
        "              prompt = f\"\"\"\n",
        "              Please provide and execute a set of test cases for the following function:\n",
        "              {full_solution}\n",
        "\n",
        "              Please do not include natural language or anything that cannot be compiled/executed.\n",
        "              Please only provided the test cases and their immediate execution.\n",
        "\n",
        "              Example:\n",
        "              def test_hello_with_name():\n",
        "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
        "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
        "              test_hello_with_name()\n",
        "\n",
        "              def test_hello_without_name():\n",
        "                  assert hello(None) == \"Hello, world\"\n",
        "                  assert hello(\"\") == \"Hello, world\"\n",
        "              test_hello_without_name()\n",
        "              \"\"\"\n",
        "              generated_tests = \"\"\n",
        "              if model_type == \"deepseek\":\n",
        "                  generated_tests = generate_code(\n",
        "                      model,\n",
        "                      tokenizer,\n",
        "                      prompt,\n",
        "                      max_new_tokens=4096\n",
        "                  )\n",
        "              elif model_type == \"semcoder\":\n",
        "                  generated_tests = model.generate_code(prompt, max_new_tokens=4096)\n",
        "\n",
        "              cleaned_tests = clean_deepseek_generated_code(generated_tests) if model_type == \"deepseek\" else \"\" #no-op for now\n",
        "              result = evaluate_single_test_suite(full_solution, cleaned_tests)\n",
        "\n",
        "              f.write(f\"PROBLEM {i}:\\n\")\n",
        "              print(f\"PROBLEM {i}:\\n\")\n",
        "              f.write(\"CANONICAL SOLUTION:\\n\")\n",
        "              print(\"CANONICAL SOLUTION:\\n\")\n",
        "              f.write(full_solution + \"\\n\")\n",
        "              print(full_solution + \"\\n\")\n",
        "              f.write(\"GENERATED TESTS:\\n\")\n",
        "              print(\"GENERATED TESTS:\\n\")\n",
        "              f.write(generated_tests + \"\\n\")\n",
        "              print(generated_tests)\n",
        "              f.write(\"CLEANED TESTS:\\n\")\n",
        "              print(\"CLEANED TESTS:\\n\")\n",
        "              f.write(cleaned_tests + \"\\n\")\n",
        "              print(cleaned_tests)\n",
        "              f.write(\"RESULT:\\n\" + str(result) + \"\\n\")\n",
        "              print(\"RESULT:\\n\" + str(result))\n",
        "\n",
        "              results.append(result)\n",
        "\n",
        "          # Calculate aggregate metrics\n",
        "          metrics[\"syntax_validity\"] = np.mean([r[\"syntax_valid\"] for r in results])\n",
        "          metrics[\"execution_accuracy\"] = np.mean([r[\"execution_success\"] for r in results])\n",
        "          f.write(str(metrics))\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "fAzhBR72brWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = TestCaseEvaluator()"
      ],
      "metadata": {
        "id": "yvhHTDR-kRwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluator.evaluate_model(model, \"deepseek\", tokenizer, 100)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "from google.colab import files\n",
        "files.download('deepseek_test_case_generation_results.txt')"
      ],
      "metadata": {
        "id": "pZgl0946phra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardized SemCoder Results"
      ],
      "metadata": {
        "id": "YaormCswUMDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluator.evaluate_model(semcoder, \"semcoder\", tokenizer, 100)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "from google.colab import files\n",
        "files.download('semcoder_test_case_generation_results.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G5u-CaLrUTpL",
        "outputId": "018d09ad-148e-4edf-923e-7e10f4397ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 0:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = abs(elem - elem2)\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "\n",
            "GENERATED TESTS:\n",
            "\n",
            "\n",
            "              Please provide and execute a set of test cases for the following function:\n",
            "              from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = abs(elem - elem2)\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "\n",
            "              Please do not include natural language or anything that cannot be compiled/executed.\n",
            "              Please only provided the test cases and their immediate execution.\n",
            "\n",
            "              Example:\n",
            "              def test_hello_with_name():\n",
            "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
            "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
            "              test_hello_with_name()\n",
            "\n",
            "              def test_hello_without_name():\n",
            "                  assert hello(None) == \"Hello, world\"\n",
            "                  assert hello(\"\") == \"Hello, world\"\n",
            "              test_hello_without_name()\n",
            "              \n",
            "CLEANED TESTS:\n",
            "\n",
            "\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 1:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "    result = []\n",
            "    current_string = []\n",
            "    current_depth = 0\n",
            "\n",
            "    for c in paren_string:\n",
            "        if c == '(':\n",
            "            current_depth += 1\n",
            "            current_string.append(c)\n",
            "        elif c == ')':\n",
            "            current_depth -= 1\n",
            "            current_string.append(c)\n",
            "\n",
            "            if current_depth == 0:\n",
            "                result.append(''.join(current_string))\n",
            "                current_string.clear()\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "GENERATED TESTS:\n",
            "\n",
            "\n",
            "              Please provide and execute a set of test cases for the following function:\n",
            "              from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "    result = []\n",
            "    current_string = []\n",
            "    current_depth = 0\n",
            "\n",
            "    for c in paren_string:\n",
            "        if c == '(':\n",
            "            current_depth += 1\n",
            "            current_string.append(c)\n",
            "        elif c == ')':\n",
            "            current_depth -= 1\n",
            "            current_string.append(c)\n",
            "\n",
            "            if current_depth == 0:\n",
            "                result.append(''.join(current_string))\n",
            "                current_string.clear()\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "              Please do not include natural language or anything that cannot be compiled/executed.\n",
            "              Please only provided the test cases and their immediate execution.\n",
            "\n",
            "              Example:\n",
            "              def test_hello_with_name():\n",
            "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
            "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
            "              test_hello_with_name()\n",
            "\n",
            "              def test_hello_without_name():\n",
            "                  assert hello(None) == \"Hello, world\"\n",
            "                  assert hello(\"\") == \"Hello, world\"\n",
            "              test_hello_without_name()\n",
            "              \n",
            "CLEANED TESTS:\n",
            "\n",
            "\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 2:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "    return number % 1.0\n",
            "\n",
            "\n",
            "GENERATED TESTS:\n",
            "\n",
            "\n",
            "              Please provide and execute a set of test cases for the following function:\n",
            "              \n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "    return number % 1.0\n",
            "\n",
            "\n",
            "              Please do not include natural language or anything that cannot be compiled/executed.\n",
            "              Please only provided the test cases and their immediate execution.\n",
            "\n",
            "              Example:\n",
            "              def test_hello_with_name():\n",
            "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
            "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
            "              test_hello_with_name()\n",
            "\n",
            "              def test_hello_without_name():\n",
            "                  assert hello(None) == \"Hello, world\"\n",
            "                  assert hello(\"\") == \"Hello, world\"\n",
            "              test_hello_without_name()\n",
            "              \n",
            "CLEANED TESTS:\n",
            "\n",
            "\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-7ff308e44fc3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemcoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"semcoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{metric}: {value:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'semcoder_test_case_generation_results.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-125da5b34cdb>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model, model_type, tokenizer, n_tasks)\u001b[0m\n\u001b[1;32m    103\u001b[0m                   )\n\u001b[1;32m    104\u001b[0m               \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"semcoder\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                   \u001b[0mgenerated_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m               \u001b[0mcleaned_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_generated_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a45ccccdd31e>\u001b[0m in \u001b[0;36mgenerate_code\u001b[0;34m(self, prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Generate with specified parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             outputs = self.model.generate(\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 )\n\u001b[1;32m    944\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    356\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Code Coverage Assessment"
      ],
      "metadata": {
        "id": "m12pf6IXmQmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install pytest pytest-cov coverage\n",
        "from google.colab import files  # Colab-specific import"
      ],
      "metadata": {
        "id": "tk8dFxQ-vTE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import subprocess\n",
        "import statistics\n",
        "from typing import Dict, List, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "from google.colab import files  # Colab-specific import"
      ],
      "metadata": {
        "id": "BTyDl0LyC8eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_aggregate_metrics(results, target_score_name) -> Dict:\n",
        "    if not results:\n",
        "        return {'error': 'No valid results to analyze'}\n",
        "\n",
        "    score_values = [r[target_score_name] for r in results if target_score_name in r]\n",
        "\n",
        "    if not score_values:\n",
        "        return {'error': 'No valid score values found'}\n",
        "\n",
        "    return {\n",
        "        f'mean_{target_score_name}': statistics.mean(score_values),\n",
        "        f'median_{target_score_name}': statistics.median(score_values),\n",
        "        f'min_{target_score_name}': min(score_values),\n",
        "        f'max_{target_score_name}': max(score_values),\n",
        "        f'std_dev': statistics.stdev(score_values) if len(score_values) > 1 else 0,\n",
        "        'total_entries_analyzed': len(score_values)\n",
        "    }"
      ],
      "metadata": {
        "id": "WDF3ZnCLBdLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCoverageAnalyzer:\n",
        "    def __init__(self, input_file: str = \"\", output_dir: str = \"/content/coverage_results\"):\n",
        "        \"\"\"Initialize the analyzer with input file path and output directory.\"\"\"\n",
        "        self.input_file = input_file\n",
        "        self.output_dir = output_dir\n",
        "        self.coverage_results = []\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def create_test_files(self, solution: str, tests: str, temp_dir: str) -> Tuple[str, str]:\n",
        "        \"\"\"Create temporary Python files for the solution and tests.\"\"\"\n",
        "        # Create solution file\n",
        "        solution_file = Path(temp_dir) / \"solution.py\"\n",
        "        with open(solution_file, 'w') as f:\n",
        "            f.write(solution)\n",
        "\n",
        "        # Create test file with proper imports for Colab\n",
        "        test_file = Path(temp_dir) / \"test_solution.py\"\n",
        "        with open(test_file, 'w') as f:\n",
        "            f.write(\"import sys\\n\")\n",
        "            f.write(f\"sys.path.append('{temp_dir}')\\n\")\n",
        "            f.write(\"from solution import *\\n\")\n",
        "            f.write(tests)\n",
        "\n",
        "        return str(solution_file), str(test_file)\n",
        "\n",
        "    def get_coverage_data():\n",
        "        with open('coverage.json') as f:\n",
        "            coverage_data = json.load(f)\n",
        "            for file_path, file_data in coverage_data['files'].items():\n",
        "                  if 'solution.py' in file_path:\n",
        "                     return {\n",
        "                        'line_coverage': file_data['summary']['percent_covered'],\n",
        "                        'total_lines': file_data['summary']['num_statements'],\n",
        "                        'covered_lines': file_data['summary']['covered_lines'],\n",
        "                        'missing_lines': file_data['summary']['missing_lines']\n",
        "                     }\n",
        "    def run_coverage_analysis(self, solution_file: str, test_file: str, temp_dir: str) -> Dict:\n",
        "        \"\"\"Run pytest with coverage and return results.\"\"\"\n",
        "        try:\n",
        "            orig_dir = os.getcwd()\n",
        "            os.chdir(temp_dir)\n",
        "\n",
        "            # Run pytest with coverage using python -m to ensure proper module resolution\n",
        "            cmd = ['python3', '-m', 'pytest', '--cov=solution',\n",
        "                '--cov-report=json', 'test_solution.py', '-v']\n",
        "\n",
        "            env = os.environ.copy()\n",
        "            env['PYTHONPATH'] = temp_dir  # Ensure proper module resolution\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
        "            if os.path.exists('coverage.json'): return get_coverage_data()\n",
        "            return {'error': 'No coverage data generated'}\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Command output: {e.output}\")\n",
        "            return {'error': f'pytest failed: {str(e)}'}\n",
        "        except Exception as e:\n",
        "            print(f\"Exception details: {str(e)}\")\n",
        "            return {'error': f'Analysis failed: {str(e)}'}\n",
        "        finally:\n",
        "            os.chdir(orig_dir)\n",
        "\n",
        "    def analyze_all_entries(self) -> Dict:\n",
        "        with open(self.input_file, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        entries = content.split('CANONICAL SOLUTION:')[1:]  # Skip first empty split\n",
        "\n",
        "        for i, entry in enumerate(entries):\n",
        "            try:\n",
        "                # Add back the header since we split on it\n",
        "                entry = 'CANONICAL SOLUTION:' + entry\n",
        "\n",
        "                with tempfile.TemporaryDirectory() as temp_dir:\n",
        "                    solution, tests = extract_sections(entry)\n",
        "                    if not tests.strip():\n",
        "                        continue\n",
        "\n",
        "                    solution_file, test_file = self.create_test_files(solution, tests, temp_dir)\n",
        "\n",
        "                    result = self.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "                    print(result)\n",
        "                    if 'line_coverage' in result:\n",
        "                        self.coverage_results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing entry {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return calculate_aggregate_metrics(self.coverage_results, \"line_coverage\")"
      ],
      "metadata": {
        "id": "B8o_-_5imP1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coverage_analyzer = TestCoverageAnalyzer()\n",
        "deep_seek_coverage_results = []\n",
        "for index, test_suite in enumerate(deepseek_7b_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    solution_file, test_file = coverage_analyzer.create_test_files(solution, test_suite, temp_dir)\n",
        "    result = coverage_analyzer.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "    if 'line_coverage' in result:\n",
        "      deep_seek_coverage_results.append(result)\n",
        "print(calculate_aggregate_metrics(deep_seek_coverage_results, \"line_coverage\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "332LRk76v3tX",
        "outputId": "616255b0-e6f0-4dd1-c546-4ebf6837880f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_line_coverage': 97.34693877551021, 'median_line_coverage': 100.0, 'min_line_coverage': 21.428571428571427, 'max_line_coverage': 100.0, 'std_dev': 13.428673596709753, 'total_entries_analyzed': 35}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_coverage_results = []\n",
        "for index, test_suite in enumerate(semcoder_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    solution_file, test_file = coverage_analyzer.create_test_files(solution, test_suite, temp_dir)\n",
        "    result = coverage_analyzer.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "    if 'line_coverage' in result:\n",
        "      semcoder_coverage_results.append(result)\n",
        "print(calculate_aggregate_metrics(semcoder_coverage_results, \"line_coverage\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_5bWgW1UYN",
        "outputId": "8bce61a6-94de-4de7-ff40-a3fc97cc00a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_line_coverage': 96.75324675324676, 'median_line_coverage': 100.0, 'min_line_coverage': 21.428571428571427, 'max_line_coverage': 100.0, 'std_dev': 14.40695307294402, 'total_entries_analyzed': 33}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measuring Novelty and Diversity"
      ],
      "metadata": {
        "id": "J2jYSvx_8ZPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring with LLM as Judge"
      ],
      "metadata": {
        "id": "hCC-fAzp8feE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "wnszaEDm_-Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "import json\n",
        "from google.colab import userdata\n",
        "def analyze_novelty_with_claude(source_function: str, generated_tests: str, original_tests: str = None) -> dict:\n",
        "    \"\"\"Use Claude API to analyze test novelty.\"\"\"\n",
        "\n",
        "    anthropic = Anthropic(api_key=userdata.get('ANTHROPIC_API_KEY'))\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "As an expert test engineer, analyze the semantic novelty and diversity of the generated test cases for the given function. Consider the function's purpose, edge cases, and expected behaviors.\n",
        "\n",
        "Source Function:\n",
        "\n",
        "{source_function}\n",
        "\n",
        "\n",
        "Generated Test Suite:\n",
        "\n",
        "{generated_tests}\n",
        "\n",
        "Original Test Suite:\n",
        "\n",
        "{original_tests}\n",
        "\n",
        "Please analyze:\n",
        "1. How well do the tests cover different aspects of the function's behavior?\n",
        "2. What novel testing scenarios are introduced?\n",
        "3. Are there important edge cases or boundary conditions tested?\n",
        "4. How diverse are the test inputs and scenarios?\n",
        "5. Are the tests relevant to the function's purpose?\n",
        "\n",
        "Provide your analysis in the following JSON format:\n",
        "{{\n",
        "    \"novelty_score\": <float between 0.0 and 1.0>,\n",
        "    \"novel_aspects\": [<list of strings describing novel aspects>],\n",
        "    \"unique_scenarios\": [<list of strings describing unique test scenarios>],\n",
        "    \"coverage_assessment\": <string describing overall test coverage>,\n",
        "    \"recommendations\": [<list of strings with suggested additional test cases>]\n",
        "}}\n",
        "Do not provide any other additonal text other than the JSON in order to facilitate\n",
        "text processing.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    message = anthropic.messages.create(\n",
        "        model=\"claude-3-sonnet-20240229\",\n",
        "        max_tokens=4096,\n",
        "        temperature=0,  # Use 0 for consistent analysis\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Parse the response as JSON\n",
        "        analysis = json.loads(message.content[0].text)\n",
        "        return analysis\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Failed to parse Claude's response as JSON\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ZCBXGynx8R20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_seek_novelty_results = []\n",
        "for index, test_suite in enumerate(deepseek_7b_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  original_tests = dataset['test'][\"test\"][index]\n",
        "  result = analyze_novelty_with_claude(solution, test_suite, original_tests)\n",
        "  print(result)\n",
        "  deep_seek_novelty_results.append(result)\n",
        "print(calculate_aggregate_metrics(deep_seek_novelty_results, \"novelty_score\"))"
      ],
      "metadata": {
        "id": "sKhY4GiQ31oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_aggregate_metrics(deep_seek_novelty_results[:34], \"novelty_score\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y3so-aA8DGJ",
        "outputId": "67cf5d25-3f89-4658-b489-11a6bc78fad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_novelty_score': 0.6558823529411765, 'median_novelty_score': 0.7, 'min_novelty_score': 0.4, 'max_novelty_score': 0.8, 'std_dev': 0.07859052479933758, 'total_entries_analyzed': 34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_novelty_results = []\n",
        "for index, test_suite in enumerate(semcoder_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  original_tests = dataset['test'][\"test\"][index]\n",
        "  result = analyze_novelty_with_claude(solution, test_suite, original_tests)\n",
        "  semcoder_novelty_results.append(result)\n",
        "  print(result)\n",
        "print(calculate_aggregate_metrics(semcoder_novelty_results, \"novelty_score\"))"
      ],
      "metadata": {
        "id": "NrlQnQ5D35nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Assuming deep_seek_novelty_results and semcoder_novelty_results are lists of dictionaries\n",
        "# as produced by your analyze_novelty_with_claude function.\n",
        "\n",
        "\n",
        "def write_results_to_file(results, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "\n",
        "write_results_to_file(deep_seek_novelty_results, 'deep_seek_novelty_results.json')\n",
        "write_results_to_file(semcoder_novelty_results, 'semcoder_novelty_results.json')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('deep_seek_novelty_results.json')\n",
        "files.download('semcoder_novelty_results.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OTQS9FXJ4y20",
        "outputId": "c063ceca-bcff-4b80-aeee-adfe30f344cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c1c14d0e-ef9f-4b16-b82e-8b904e2bbe00\", \"deep_seek_novelty_results.json\", 33777)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79476efe-fc40-411d-9e24-8ba4be834f83\", \"semcoder_novelty_results.json\", 30099)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring with Patterns"
      ],
      "metadata": {
        "id": "pNPwVBMn7ZLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class CoveragePatternAnalyzer:\n",
        "    \"\"\"Analyzes test coverage patterns focusing on types of test cases.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.patterns = {\n",
        "            'edge_cases': {\n",
        "                'empty_input': r'assert.*(?:empty|\\[\\]|\\{\\}|\\(\\)|\"\"|\\'\\'|\\b==\\s*\\[\\]|\\b==\\s*\"\"|\\b==\\s*\\'\\')',\n",
        "                'null_input': r'assert.*(?:None|null)',\n",
        "                'single_element': r'assert.*\\[[^,\\]]+\\]'\n",
        "            },\n",
        "            'boundary_testing': {\n",
        "                'zero_values': r'assert.*\\b0\\b',\n",
        "                'negative_values': r'assert.*-\\d+',\n",
        "                'large_values': r'assert.*\\d{5,}'\n",
        "            },\n",
        "            'error_handling': {\n",
        "                'exception_testing': r'with\\s+pytest\\.raises\\([^)]+\\)',\n",
        "                'invalid_input': r'assert.*(invalid|wrong|incorrect|bad)'\n",
        "            },\n",
        "            'functionality': {\n",
        "                'typical_case': r'assert.*normal|typical|standard',\n",
        "                'complex_input': r'assert.*(?:\\[.*,.*,.*\\]|\\{.*:.*,.*:.*\\}|\\(.*,.*,.*\\))'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _extract_assertions(self, test_code: str) -> List[str]:\n",
        "        \"\"\"Extract assertions with improved handling of multi-line and truncated assertions.\"\"\"\n",
        "        lines = test_code.split('\\n')\n",
        "        assertions = []\n",
        "        current_assertion = None\n",
        "        in_raises_block = False\n",
        "        bracket_count = 0\n",
        "        paren_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Skip empty lines\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Start of pytest.raises block\n",
        "            if 'pytest.raises' in line:\n",
        "                in_raises_block = True\n",
        "                current_assertion = line\n",
        "                paren_count = line.count('(') - line.count(')')\n",
        "                if paren_count == 0:\n",
        "                    assertions.append(current_assertion)\n",
        "                    current_assertion = None\n",
        "                    in_raises_block = False\n",
        "                continue\n",
        "\n",
        "            # Start of regular assertion\n",
        "            if line.startswith('assert'):\n",
        "                current_assertion = line\n",
        "                bracket_count = line.count('[') - line.count(']')\n",
        "                paren_count = line.count('(') - line.count(')')\n",
        "                if bracket_count == 0 and paren_count == 0:\n",
        "                    assertions.append(current_assertion)\n",
        "                    current_assertion = None\n",
        "                continue\n",
        "\n",
        "            # Continue previous assertion\n",
        "            if current_assertion:\n",
        "                current_assertion += ' ' + line\n",
        "                if in_raises_block:\n",
        "                    paren_count += line.count('(') - line.count(')')\n",
        "                    if paren_count == 0:\n",
        "                        assertions.append(current_assertion)\n",
        "                        current_assertion = None\n",
        "                        in_raises_block = False\n",
        "                else:\n",
        "                    bracket_count += line.count('[') - line.count(']')\n",
        "                    paren_count += line.count('(') - line.count(')')\n",
        "                    if bracket_count == 0 and paren_count == 0:\n",
        "                        assertions.append(current_assertion)\n",
        "                        current_assertion = None\n",
        "\n",
        "        # Handle any remaining incomplete assertion\n",
        "        if current_assertion:\n",
        "            assertions.append(current_assertion + ' ...')\n",
        "\n",
        "        return assertions\n",
        "\n",
        "    def analyze_test_suite(self, test_code: str) -> Dict:\n",
        "        \"\"\"Analyze a test suite and return detailed coverage metrics.\"\"\"\n",
        "        assertions = self._extract_assertions(test_code)\n",
        "        for assertion in assertions:\n",
        "          print(assertion)\n",
        "        total_assertions = len(assertions)\n",
        "        if total_assertions == 0:\n",
        "            return {'error': 'No assertions found'}\n",
        "\n",
        "        # Track which patterns match each assertion\n",
        "        assertion_patterns = {i: set() for i in range(total_assertions)}\n",
        "        pattern_counts = defaultdict(lambda: defaultdict(int))\n",
        "        uncategorized_assertions = []\n",
        "\n",
        "        # Analyze each assertion\n",
        "        for i, assertion in enumerate(assertions):\n",
        "            matches_found = False\n",
        "            for category, patterns in self.patterns.items():\n",
        "                for name, pattern in patterns.items():\n",
        "                    if re.search(pattern, assertion):\n",
        "                        assertion_patterns[i].add(f\"{category}:{name}\")\n",
        "                        pattern_counts[category][name] += 1\n",
        "                        matches_found = True\n",
        "\n",
        "            if not matches_found:\n",
        "                uncategorized_assertions.append(assertion)\n",
        "\n",
        "        # Calculate metrics\n",
        "        results = {}\n",
        "        for category, patterns in pattern_counts.items():\n",
        "            category_assertions = len([i for i in assertion_patterns.values()\n",
        "                                    if any(p.startswith(f\"{category}:\") for p in i)])\n",
        "            results[category] = {\n",
        "                'total_matches': category_assertions,\n",
        "                'coverage_ratio': category_assertions / total_assertions,\n",
        "                'pattern_breakdown': dict(patterns)\n",
        "            }\n",
        "\n",
        "        # Add overall metrics\n",
        "        results['overall'] = {\n",
        "            'total_assertions': total_assertions,\n",
        "            'patterns_per_assertion': sum(len(p) for p in assertion_patterns.values()) / total_assertions,\n",
        "            'pattern_coverage': len([p for p in sum([list(p.values()) for p in pattern_counts.values()], []) if p > 0]) / \\\n",
        "                              len(sum([list(p.values()) for p in self.patterns.values()], [])),\n",
        "            'uncategorized': len(uncategorized_assertions),\n",
        "            'uncategorized_assertions': uncategorized_assertions\n",
        "        }\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "GxuebA5STYbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_analyzer = CoveragePatternAnalyzer()"
      ],
      "metadata": {
        "id": "NlySCWNo5cna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_extracted_test_suites_str = \"\\n\\n\".join(deepseek_7b_extracted_test_suites)"
      ],
      "metadata": {
        "id": "Sn43flYS8cv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_seek_pattern_analyzer_results = pattern_analyzer.analyze_test_suite(deepseek_extracted_test_suites_str)"
      ],
      "metadata": {
        "id": "lY9xpGGY8SNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in deep_seek_pattern_analyzer_results:\n",
        "  print(item)\n",
        "  print(deep_seek_pattern_analyzer_results[item])"
      ],
      "metadata": {
        "id": "q3_OtH6G_buo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_extracted_test_suites_str = \"\\n\\n\".join(semcoder_extracted_test_suites)"
      ],
      "metadata": {
        "id": "qu52Ve05-2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_pattern_analyzer_results = pattern_analyzer.analyze_test_suite(semcoder_extracted_test_suites_str)"
      ],
      "metadata": {
        "id": "b3oktFKL-6lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in semcoder_pattern_analyzer_results:\n",
        "  print(item)\n",
        "  print(semcoder_pattern_analyzer_results[item])"
      ],
      "metadata": {
        "id": "2gbCoWfUHGq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-4 Results"
      ],
      "metadata": {
        "id": "n-zTCUcPqVG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28 #the latest version is acting crazy weird- ugh rollback"
      ],
      "metadata": {
        "id": "J7UIRuVY5yUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_humaneval_plus_tests(\"gpt-4\", deep_seek_tokenizer=None, num_total_tests=100) #gpt-4 doesn't have reasoning training; only gpt-4o- interesting case comparison"
      ],
      "metadata": {
        "id": "RqivJLJgt7Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4_extracted_test_suites = process_file_path(\"/content/gpt-4_test_case_generation_results.txt\")"
      ],
      "metadata": {
        "id": "RnPzHwt--LKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_test_suite(\"gpt-4\",dataset, 21, gpt_4_extracted_test_suites)"
      ],
      "metadata": {
        "id": "sR92q3lV_0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4_coverage_analyzer = TestCoverageAnalyzer()\n",
        "gpt_4_coverage_results = []\n",
        "for index, test_suite in enumerate(gpt_4_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    solution_file, test_file = gpt_4_coverage_analyzer.create_test_files(solution, test_suite, temp_dir)\n",
        "    result = gpt_4_coverage_analyzer.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "    if 'line_coverage' in result:\n",
        "      gpt_4_coverage_results.append(result)\n",
        "print(calculate_aggregate_metrics(gpt_4_coverage_results, \"line_coverage\"))"
      ],
      "metadata": {
        "id": "tsGeq_KU-XjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4_novelty_results = []\n",
        "for index, test_suite in enumerate(gpt_4_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  original_tests = dataset['test'][\"test\"][index]\n",
        "  result = analyze_novelty_with_claude(solution, test_suite, original_tests)\n",
        "  print(result)\n",
        "  gpt_4_novelty_results.append(result)\n",
        "print(calculate_aggregate_metrics(gpt_4_novelty_results, \"novelty_score\"))"
      ],
      "metadata": {
        "id": "B0r4yMKr_Lk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_results_to_file(gpt_4_novelty_results, 'gpt_4_novelty_results.json')\n",
        "files.download('gpt_4_novelty_results.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lf9GWmVy_JFK",
        "outputId": "505c4fa2-4c37-457f-bc10-e4457f068ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0815b87a-ebf2-4bb0-b29c-ac2260a5cca9\", \"gpt_4_novelty_results.json\", 21486)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}